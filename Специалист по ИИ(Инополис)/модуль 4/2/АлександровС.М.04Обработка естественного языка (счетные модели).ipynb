{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-05-13T13:37:00.833018Z",
     "start_time": "2023-05-13T13:37:00.547151Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/nascela/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/nascela/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd # библиотека для удобной работы с датафреймами\n",
    "\n",
    "# библиотека, где реализованы основные алгоритмы машинного обучения\n",
    "from sklearn.metrics import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression # можно заменить на любой другой классификатор\n",
    "from sklearn.feature_extraction.text import CountVectorizer # Модель \"мешка слов\", см. далее\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk # уже знакомая нам библиотека nltk\n",
    "from nltk.tokenize import word_tokenize # готовый токенизатор библиотеки nltk\n",
    "\n",
    "# импортируем стоп-слова из библиотеки nltk\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "noise = stopwords.words('russian') + list(punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4b/wpq4f85s0lzcrjpl4chzpqpw0000gn/T/ipykernel_34662/3949273962.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = positive.append(negative)\n"
     ]
    }
   ],
   "source": [
    "# загружаем положительные твиты\n",
    "positive = pd.read_csv('positive.csv', sep=';', usecols=[3], names=['text'])\n",
    "positive['label'] = ['positive'] * len(positive) # устанавливаем метки\n",
    "\n",
    "# загружаем отрицательные твиты\n",
    "negative = pd.read_csv('negative.csv', sep=';', usecols=[3], names=['text'])\n",
    "negative['label'] = ['negative'] * len(negative) # устанавливаем метки\n",
    "\n",
    "# соединяем вместе\n",
    "df = positive.append(negative)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-13T10:19:41.155139Z",
     "start_time": "2023-05-13T10:19:40.671186Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(df.text, df.label, test_size=0.2, random_state=42)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-13T10:20:17.872773Z",
     "start_time": "2023-05-13T10:20:17.848966Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.76      0.77      0.77     22480\n",
      "    positive       0.77      0.77      0.77     22887\n",
      "\n",
      "    accuracy                           0.77     45367\n",
      "   macro avg       0.77      0.77      0.77     45367\n",
      "weighted avg       0.77      0.77      0.77     45367\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(ngram_range=(1, 1))\n",
    "\n",
    "vectorized_x_train = vectorizer.fit_transform(x_train)\n",
    "vectorized_x_test = vectorizer.transform(x_test)\n",
    "\n",
    "clf = LogisticRegression(random_state=42, max_iter=1000) # фиксируем random_state для воспроизводимости результатов\n",
    "clf.fit(vectorized_x_train, y_train)\n",
    "\n",
    "pred = clf.predict(vectorized_x_test)\n",
    "\n",
    "print(classification_report(y_test, pred))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-13T14:03:29.661935Z",
     "start_time": "2023-05-13T14:03:14.742656Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.74      0.75     22480\n",
      "    positive       0.75      0.78      0.77     22887\n",
      "\n",
      "    accuracy                           0.76     45367\n",
      "   macro avg       0.76      0.76      0.76     45367\n",
      "weighted avg       0.76      0.76      0.76     45367\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# инициализируем векторизатор, в качестве переменных используем униграммы\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 1))\n",
    "\n",
    "# обучаем его и сразу применяем к x_train\n",
    "tfidf_vectorized_x_train = tfidf_vectorizer.fit_transform(x_train)\n",
    "# применяем обученный векторизатор к тестовым данным\n",
    "tfidf_vectorized_x_test = tfidf_vectorizer.transform(x_test)\n",
    "\n",
    "# инициализируем и обучаем классификатор\n",
    "clf = LogisticRegression(random_state=88, max_iter=1000)\n",
    "clf.fit(tfidf_vectorized_x_train, y_train)\n",
    "\n",
    "# применяем обученный векторизатор к тестовым данным\n",
    "tfidf_vectorized_x_test = tfidf_vectorizer.transform(x_test)\n",
    "\n",
    "# получаем предсказания и выводим информацию о качестве\n",
    "pred = clf.predict(tfidf_vectorized_x_test)\n",
    "print(classification_report(y_test, pred))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-13T11:00:46.287005Z",
     "start_time": "2023-05-13T11:00:37.619057Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nascela/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/nascela/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['``'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.80      0.79     22480\n",
      "    positive       0.80      0.76      0.78     22887\n",
      "\n",
      "    accuracy                           0.78     45367\n",
      "   macro avg       0.78      0.78      0.78     45367\n",
      "weighted avg       0.78      0.78      0.78     45367\n",
      "\n"
     ]
    }
   ],
   "source": [
    "smart_vectorizer = CountVectorizer(ngram_range=(1, 1),\n",
    "                                   tokenizer=word_tokenize,\n",
    "                                   stop_words=noise)\n",
    "# обучаем его и сразу применяем к x_train\n",
    "smart_vectorized_x_train = smart_vectorizer.fit_transform(x_train)\n",
    "\n",
    "# инициализируем и обучаем классификатор\n",
    "clf = LogisticRegression(random_state=88, max_iter=1000)\n",
    "clf.fit(smart_vectorized_x_train, y_train)\n",
    "\n",
    "# применяем обученный векторайзер к тестовым данным\n",
    "smart_vectorized_x_test = smart_vectorizer.transform(x_test)\n",
    "\n",
    "# получаем предсказания и выводим информацию о качестве\n",
    "pred = clf.predict(smart_vectorized_x_test)\n",
    "print(classification_report(y_test, pred))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-13T13:23:02.157517Z",
     "start_time": "2023-05-13T13:22:34.812602Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nascela/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/nascela/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['``'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.82      0.79     22480\n",
      "    positive       0.81      0.76      0.79     22887\n",
      "\n",
      "    accuracy                           0.79     45367\n",
      "   macro avg       0.79      0.79      0.79     45367\n",
      "weighted avg       0.79      0.79      0.79     45367\n",
      "\n"
     ]
    }
   ],
   "source": [
    "smart_vectorizer = CountVectorizer(ngram_range=(1, 2),\n",
    "                                   tokenizer=word_tokenize,\n",
    "                                   stop_words=noise)\n",
    "# обучаем его и сразу применяем к x_train\n",
    "smart_vectorized_x_train = smart_vectorizer.fit_transform(x_train)\n",
    "\n",
    "# инициализируем и обучаем классификатор\n",
    "clf = LogisticRegression(random_state=88, max_iter=1000)\n",
    "clf.fit(smart_vectorized_x_train, y_train)\n",
    "\n",
    "# применяем обученный векторайзер к тестовым данным\n",
    "smart_vectorized_x_test = smart_vectorizer.transform(x_test)\n",
    "\n",
    "# получаем предсказания и выводим информацию о качестве\n",
    "pred = clf.predict(smart_vectorized_x_test)\n",
    "print(classification_report(y_test, pred))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-13T13:39:15.096985Z",
     "start_time": "2023-05-13T13:38:16.019571Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nascela/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/nascela/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['``'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.63      0.82      0.71     22480\n",
      "    positive       0.75      0.53      0.62     22887\n",
      "\n",
      "    accuracy                           0.67     45367\n",
      "   macro avg       0.69      0.68      0.67     45367\n",
      "weighted avg       0.69      0.67      0.67     45367\n",
      "\n"
     ]
    }
   ],
   "source": [
    "smart_vectorizer = CountVectorizer(ngram_range=(2, 2),\n",
    "                                   tokenizer=word_tokenize,\n",
    "                                   stop_words=noise)\n",
    "# обучаем его и сразу применяем к x_train\n",
    "smart_vectorized_x_train = smart_vectorizer.fit_transform(x_train)\n",
    "\n",
    "# инициализируем и обучаем классификатор\n",
    "clf = LogisticRegression(random_state=88, max_iter=1000)\n",
    "clf.fit(smart_vectorized_x_train, y_train)\n",
    "\n",
    "# применяем обученный векторайзер к тестовым данным\n",
    "smart_vectorized_x_test = smart_vectorizer.transform(x_test)\n",
    "\n",
    "# получаем предсказания и выводим информацию о качестве\n",
    "pred = clf.predict(smart_vectorized_x_test)\n",
    "print(classification_report(y_test, pred))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-13T13:42:16.027215Z",
     "start_time": "2023-05-13T13:41:36.871065Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nascela/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/nascela/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['``'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.52      1.00      0.68     22480\n",
      "    positive       0.99      0.10      0.18     22887\n",
      "\n",
      "    accuracy                           0.54     45367\n",
      "   macro avg       0.76      0.55      0.43     45367\n",
      "weighted avg       0.76      0.54      0.43     45367\n",
      "\n"
     ]
    }
   ],
   "source": [
    "smart_vectorizer = CountVectorizer(ngram_range=(5, 5),\n",
    "                                   tokenizer=word_tokenize,\n",
    "                                   stop_words=noise)\n",
    "# обучаем его и сразу применяем к x_train\n",
    "smart_vectorized_x_train = smart_vectorizer.fit_transform(x_train)\n",
    "\n",
    "# инициализируем и обучаем классификатор\n",
    "clf = LogisticRegression(random_state=88, max_iter=1000)\n",
    "clf.fit(smart_vectorized_x_train, y_train)\n",
    "\n",
    "# применяем обученный векторайзер к тестовым данным\n",
    "smart_vectorized_x_test = smart_vectorizer.transform(x_test)\n",
    "\n",
    "# получаем предсказания и выводим информацию о качестве\n",
    "pred = clf.predict(smart_vectorized_x_test)\n",
    "print(classification_report(y_test, pred))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-13T13:52:43.446487Z",
     "start_time": "2023-05-13T13:52:15.963196Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nascela/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/nascela/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['``'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.54      0.97      0.69     22480\n",
      "    positive       0.87      0.18      0.30     22887\n",
      "\n",
      "    accuracy                           0.57     45367\n",
      "   macro avg       0.70      0.58      0.50     45367\n",
      "weighted avg       0.70      0.57      0.49     45367\n",
      "\n"
     ]
    }
   ],
   "source": [
    "smart_vectorizer = CountVectorizer(ngram_range=(3, 3),\n",
    "                                   tokenizer=word_tokenize,\n",
    "                                   stop_words=noise)\n",
    "# обучаем его и сразу применяем к x_train\n",
    "smart_vectorized_x_train = smart_vectorizer.fit_transform(x_train)\n",
    "\n",
    "# инициализируем и обучаем классификатор\n",
    "clf = LogisticRegression(random_state=88, max_iter=1000)\n",
    "clf.fit(smart_vectorized_x_train, y_train)\n",
    "\n",
    "# применяем обученный векторайзер к тестовым данным\n",
    "smart_vectorized_x_test = smart_vectorizer.transform(x_test)\n",
    "\n",
    "# получаем предсказания и выводим информацию о качестве\n",
    "pred = clf.predict(smart_vectorized_x_test)\n",
    "print(classification_report(y_test, pred))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-13T14:08:04.234601Z",
     "start_time": "2023-05-13T14:07:27.557749Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## исследование величины n в n-граммах показало, что:\n",
    "\n",
    "**Оптимальными значениями являются** - ngram_range=(1, 2), т.к. они дают самые оптимальные значения метрик\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nascela/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/nascela/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['``'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.82      0.79     22480\n",
      "    positive       0.81      0.76      0.79     22887\n",
      "\n",
      "    accuracy                           0.79     45367\n",
      "   macro avg       0.79      0.79      0.79     45367\n",
      "weighted avg       0.79      0.79      0.79     45367\n",
      "\n"
     ]
    }
   ],
   "source": [
    "smart_vectorizer = CountVectorizer(ngram_range=(1, 2),\n",
    "                                   tokenizer=word_tokenize,\n",
    "                                   stop_words=noise)\n",
    "# обучаем его и сразу применяем к x_train\n",
    "smart_vectorized_x_train = smart_vectorizer.fit_transform(x_train)\n",
    "# применяем обученный векторайзер к тестовым данным\n",
    "smart_vectorized_x_test = smart_vectorizer.transform(x_test)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_clf = LogisticRegression(solver=\"lbfgs\", max_iter=1000, random_state=42)\n",
    "\n",
    "log_clf.fit(smart_vectorized_x_train, y_train)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# получаем предсказания и выводим информацию о качестве\n",
    "pred = log_clf.predict(smart_vectorized_x_test)\n",
    "print(classification_report(y_test, pred))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-13T15:49:25.395839Z",
     "start_time": "2023-05-13T15:48:26.741691Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nascela/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/nascela/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['``'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# инициализируем и обучаем классификатор\n",
    "\n",
    "# from sklearn.neighbors import KNeighborsRegressor\n",
    "# from sklearn.linear_model import SGDClassifier\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "# ovr_clf = OneVsRestClassifier(SVC())\n",
    "# forest_clf = RandomForestClassifier(random_state=42)\n",
    "# sgd_clf = SGDClassifier(random_state=42)\n",
    "# knn_clf = KNeighborsRegressor(n_neighbors = 4, weights = 'distance')\n",
    "\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "svm_clf = SVC()\n",
    "\n",
    "\n",
    "smart_vectorizer = CountVectorizer(ngram_range=(1, 2),\n",
    "                                   tokenizer=word_tokenize,\n",
    "                                   stop_words=noise)\n",
    "# обучаем его и сразу применяем к x_train\n",
    "smart_vectorized_x_train = smart_vectorizer.fit_transform(x_train)\n",
    "# применяем обученный векторайзер к тестовым данным\n",
    "smart_vectorized_x_test = smart_vectorizer.transform(x_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "svm_clf.fit(smart_vectorized_x_train, y_train)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# получаем предсказания и выводим информацию о качестве\n",
    "pred = svm_clf.predict(smart_vectorized_x_test)\n",
    "print(classification_report(y_test, pred))\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-05-13T15:58:12.023282Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
