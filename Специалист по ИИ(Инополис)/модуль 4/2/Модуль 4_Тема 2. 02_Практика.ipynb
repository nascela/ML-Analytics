{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тема \"Обработка естественного языка (счетные модели)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KvOWdHcUWg98"
   },
   "source": [
    "## Задача: классификация твитов по тональности\n",
    "\n",
    "У нас есть выборка из твитов.\n",
    "Нам известна эмоциональная окраска каждого твита из выборки: положительная или отрицательная. Задача состоит в построении модели, которая по тексту твита предсказывает его эмоциональную окраску.\n",
    "\n",
    "Классификацию по тональности используют в рекомендательных системах, чтобы понять, понравилось ли людям кафе, кино, etc.\n",
    "\n",
    "Скачать выборку можно по ссылкам или же воспользовать уже скачанными файлами ([источник](http://study.mokoron.com/)): [положительные](https://www.dropbox.com/s/fnpq3z4bcnoktiv/positive.csv?dl=0), [отрицательные](https://www.dropbox.com/s/r6u59ljhhjdg6j0/negative.csv)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "GYvTkhGnWg98",
    "ExecuteTime": {
     "end_time": "2023-05-13T09:12:53.735712Z",
     "start_time": "2023-05-13T09:12:53.733236Z"
    }
   },
   "outputs": [],
   "source": [
    "# если вы работете в GoogleCollab, где работает wget, то данные можно скачать данные так:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "IX-AeH8RWg9-",
    "ExecuteTime": {
     "end_time": "2023-05-13T09:12:54.634554Z",
     "start_time": "2023-05-13T09:12:54.629467Z"
    }
   },
   "outputs": [],
   "source": [
    "#!wget https://www.dropbox.com/s/fnpq3z4bcnoktiv/positive.csv\n",
    "#!wget https://www.dropbox.com/s/r6u59ljhhjdg6j0/negative.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import pandas as pd # библиотека для удобной работы с датафреймами\n",
    "import numpy as np # библиотека для удобной работы со списками и матрицами\n",
    "\n",
    "# библиотека, где реализованы основные алгоритмы машинного обучения\n",
    "from sklearn.metrics import * \n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.pipeline import Pipeline"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-13T09:12:56.790176Z",
     "start_time": "2023-05-13T09:12:55.454322Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Откроем файлы и создадим массив из текстов и правильных меток для твитов.\n",
    "Сначала идут положительные твиты, потом отрицательные."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4b/wpq4f85s0lzcrjpl4chzpqpw0000gn/T/ipykernel_31246/193052043.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = positive.append(negative)\n"
     ]
    }
   ],
   "source": [
    "# загружаем положительные твиты\n",
    "positive = pd.read_csv('positive.csv', sep=';', usecols=[3], names=['text'])\n",
    "positive['label'] = ['positive'] * len(positive) # устанавливаем метки\n",
    "\n",
    "# загружаем отрицательные твиты\n",
    "negative = pd.read_csv('negative.csv', sep=';', usecols=[3], names=['text'])\n",
    "negative['label'] = ['negative'] * len(negative) # устанавливаем метки\n",
    "\n",
    "# соединяем вместе\n",
    "df = positive.append(negative)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-13T09:13:54.414304Z",
     "start_time": "2023-05-13T09:13:53.918747Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Посмотрим на полученные данные:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                     text     label\n15931   RT @Blawar_1337: Теперь у нас с @Wake_UA появи...  positive\n59532   с днём рождения зайка*))) ухх погуляем мы сего...  positive\n47185   RT @Shumkova0406199: @ann_safina Вов вов вов А...  negative\n42002   Надо выдернуть звуковую дорожку из \"Доктора Ка...  positive\n109035  @_hassliebe_ может все таки на этой неделе вер...  negative",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>15931</th>\n      <td>RT @Blawar_1337: Теперь у нас с @Wake_UA появи...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>59532</th>\n      <td>с днём рождения зайка*))) ухх погуляем мы сего...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>47185</th>\n      <td>RT @Shumkova0406199: @ann_safina Вов вов вов А...</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>42002</th>\n      <td>Надо выдернуть звуковую дорожку из \"Доктора Ка...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>109035</th>\n      <td>@_hassliebe_ может все таки на этой неделе вер...</td>\n      <td>negative</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(5, random_state=40)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-13T09:13:58.063423Z",
     "start_time": "2023-05-13T09:13:58.031889Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Разбиваем данные на обучающую и тестовую выборки с помощью функции ```train_test_split()``` из **sklearn**:\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(df.text, df.label, test_size=0.2, random_state=42)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-13T09:19:46.661624Z",
     "start_time": "2023-05-13T09:19:46.604278Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "quiUoyqNb3WA"
   },
   "source": [
    "## Baseline: классификация необработанных n-грамм\n",
    "\n",
    "* Сейчас мы попробуем получить преобразование предложений в численный вектор, с которым может работать стандартный алгоритм машинного обучения, например логистическая регрессия. \n",
    "* Для этого нам понадобится познакомиться с понятием n-gram - самых мелких элементов предложения, с которыми можно работать. \n",
    "* Подсчитав количество этих n-грам в предложениях, мы получим искомые численные представления."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Функция для работы с n-граммами реализована в библиотке **nltk** (Natural Language ToolKit), импортируем эту функцию: "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "from nltk import ngrams"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-13T09:19:57.636084Z",
     "start_time": "2023-05-13T09:19:57.197Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Прежде чем получать n-граммы, нужно разделить предложение на отдельные слова.  Для этого используем метод ```split()```."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "['Кому', 'нужен', 'ломтик', 'июльского', 'неба?']"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = 'Кому нужен ломтик июльского неба?'.split()\n",
    "sentence"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-13T09:20:04.071869Z",
     "start_time": "2023-05-13T09:20:04.061957Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Чтобы получить n-грамму для такой последовательности, используем функцию ```ngrams()```. \n",
    "\n",
    "На вход подается два параметра:\n",
    "* лист с разделенным на отдельные слова предложением (у нас он хранится в переменной ```sent```);\n",
    "* параметр n, определяющий, какой тип n-грамм мы хотим получить.\n",
    "\n",
    "\n",
    "Чтобы полученный объект отобразить, делаем из него ```list```. "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "[('Кому',), ('нужен',), ('ломтик',), ('июльского',), ('неба?',)]"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(ngrams(sentence, 1)) # униграммы"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-13T10:05:11.065421Z",
     "start_time": "2023-05-13T10:05:11.062486Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Аналогично мы можем получить биграммы - для этого заменяем параметр **n** в функции **ngrams** с 1 на 2."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "executionInfo": {
     "elapsed": 625,
     "status": "ok",
     "timestamp": 1595572084259,
     "user": {
      "displayName": "Viacheslav Ivanov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj2nVSPc3MmvxSyrr0vJY2olNGRMXHUnFlFUnxktA=s64",
      "userId": "14095316471752402401"
     },
     "user_tz": -180
    },
    "id": "nCkkFzWLWg-R",
    "outputId": "34818c20-192b-4786-fcc6-149ea0017726",
    "ExecuteTime": {
     "end_time": "2023-05-13T10:05:12.510770Z",
     "start_time": "2023-05-13T10:05:12.491964Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "[('Кому', 'нужен'),\n ('нужен', 'ломтик'),\n ('ломтик', 'июльского'),\n ('июльского', 'неба?')]"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(ngrams(sentence, 2)) # биграммы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "executionInfo": {
     "elapsed": 436,
     "status": "ok",
     "timestamp": 1595572084260,
     "user": {
      "displayName": "Viacheslav Ivanov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj2nVSPc3MmvxSyrr0vJY2olNGRMXHUnFlFUnxktA=s64",
      "userId": "14095316471752402401"
     },
     "user_tz": -180
    },
    "id": "GygS6_fJWg-S",
    "outputId": "17d4a687-d6ac-4982-c444-aa5a77de9337",
    "ExecuteTime": {
     "end_time": "2023-05-13T10:05:13.113682Z",
     "start_time": "2023-05-13T10:05:13.108202Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "[('Кому', 'нужен', 'ломтик'),\n ('нужен', 'ломтик', 'июльского'),\n ('ломтик', 'июльского', 'неба?')]"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(ngrams(sentence, 3)) # триграммы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "[('Кому', 'нужен', 'ломтик', 'июльского', 'неба?')]"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(ngrams(sentence, 5))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-13T10:05:13.887485Z",
     "start_time": "2023-05-13T10:05:13.877220Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V5hiNv2eVAc-"
   },
   "source": [
    "### Векторизаторы\n",
    "\n",
    "Векторизатор преобразует слово или набор слов в числовой вектор, понятный алгоритму машинного обучения, который привык работать с числовыми табличными данными.\n",
    "\n",
    "Ниже - пример преобразования слов в двумерных вектор, каждому слову соответствует точка на плоскости.\n",
    "\n",
    "<a href=\"https://drive.google.com/uc?id=1ukv-FTj0jeVdcgVlOaNBocUfNuYGGVZg\n",
    "\" target=\"_blank\"><img src=\"https://drive.google.com/uc?id=1ukv-FTj0jeVdcgVlOaNBocUfNuYGGVZg\" \n",
    "alt=\"IMAGE ALT TEXT HERE\" width=\"600\" border=\"0\" /></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "На начальном этапе нам будет достаточно тех инструментов, которые уже есть в знакомой нам библиотеке **sklearn**."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression # можно заменить на любой другой классификатор\n",
    "from sklearn.feature_extraction.text import CountVectorizer # Модель \"мешка слов\", см. далее"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-13T10:05:19.299309Z",
     "start_time": "2023-05-13T10:05:19.289234Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oklbwY_vWg-X"
   },
   "source": [
    "Самый простой способ извлечь признаки из текстовых данных -- векторизаторы: `CountVectorizer` и `TfidfVectorizer`\n",
    "\n",
    "Объект `CountVectorizer` делает следующее:\n",
    "* строит для каждого документа (каждой пришедшей ему строки) вектор размерности `n`, где `n` -- количество слов или n-грам во всём корпусе\n",
    "* заполняет каждый i-тый элемент количеством вхождений слова в данный документ\n",
    "\n",
    "<a href=\"https://drive.google.com/uc?id=1ukv-FTj0jeVdcgVlOaNBocUfNuYGGVZg\n",
    "\" target=\"_blank\"><img src=\"https://drive.google.com/uc?id=1jHmkrGZTMawM46Yzxh243Ur1y5pYKzrl\" \n",
    "alt=\"IMAGE ALT TEXT HERE\" width=\"600\" border=\"0\" /></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZLTEzNWxrx5X"
   },
   "source": [
    "На рисунке пример векторизации для униграмм, но можно использовать любые n-граммы. Для этого у объекта ```CountVectorizer()``` есть параметр **ngram_range**, который отвечает за то, какие n-граммы мы используем в качестве признаов:<br/>\n",
    "ngram_range=(1, 1) -- униграммы<br/>\n",
    "ngram_range=(3, 3) -- триграммы<br/>\n",
    "ngram_range=(1, 3) -- униграммы, биграммы и триграммы."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Инициализируем ```CountVectorizer()```, указав в качестве признаков униграммы:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(ngram_range=(1, 1))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-13T10:06:10.879970Z",
     "start_time": "2023-05-13T10:06:10.855639Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "После инициализации _vectorizer_ можно обучить на наших данных. \n",
    "\n",
    "Для обучения используем обучающую выборку ```x_train```, но в отличие от классификатора мы используем метод ```fit_transform()```: сначала обучаем наш векторизатор, а потом сразу применяем его к нашему набору данных. Это похоже на то, как мы работали с label encoderом и one-hot-encoderом.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "vectorized_x_train = vectorizer.fit_transform(x_train)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gyT1Kz6Ppt8n"
   },
   "source": [
    "Так как результат не зависит от порядка слов в текстах, то говорят, что такая модель представления текстов в виде векторов получается из *гипотезы представления текста как мешка слов*"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "В vectorizer.vocabulary_ лежит словарь, отображение слов в их индексы:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "list(vectorizer.vocabulary_.items())[:10]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "В нашей выборке 170125 текстов (твитов), в них встречается 243760 разных слов."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "vectorized_x_train.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Так как теперь у нас есть **численное представление** и набор входных признаков, то мы можем обучить модель логистической регрессии (или любую другую из тех, на которые мы смотрели раньше, например, случайный лес)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "clf = LogisticRegression(random_state=88, max_iter=1000) # фиксируем random_state для воспроизводимости результатов\n",
    "clf.fit(vectorized_x_train, y_train)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "С тестовыми данными нужно проделать то же самое, что и с данными для обучения: сделать из текстов вектора, которые можно передавать в классификатор для прогноза класса объекта. \n",
    "\n",
    "У нас уже есть обученный векторизатор ```vectorizer```, поэтому используем метод ```transform()``` (просто применить его), а не ```fit_transform``` (обучить и применить)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "vectorized_x_test = vectorizer.transform(x_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Как раньше, для получения прогноза у обученного классификатора используем метод ```predict()```.\n",
    "\n",
    "С помощью функции ```classification_report()```, которая считает сразу несколько метрик качества классификации, посмотрим на то, насколько хорошо мы предсказываем положительную или отрицательную тональность твита ."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pred = clf.predict(vectorized_x_test)\n",
    "print(classification_report(y_test, pred))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mjy5ZPmwWg-j"
   },
   "source": [
    "## Использование триграмм"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Попробуем сделать то же самое, используя в качестве признаков триграммы:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# инициализируем векторайзер \n",
    "trigram_vectorizer = CountVectorizer(ngram_range=(3, 3))\n",
    "\n",
    "# обучаем его и сразу применяем к x_train\n",
    "trigram_vectorized_x_train = trigram_vectorizer.fit_transform(x_train)\n",
    "# применяем обученный векторизатор к тестовым данным\n",
    "trigram_vectorized_x_test = trigram_vectorizer.transform(x_test)\n",
    "\n",
    "# инициализируем и обучаем классификатор\n",
    "clf = LogisticRegression(random_state=42, max_iter=1000)\n",
    "clf.fit(trigram_vectorized_x_train, y_train)\n",
    "\n",
    "# применяем обученный векторизатор к тестовым данным\n",
    "trigram_vectorized_x_test = trigram_vectorizer.transform(x_test)\n",
    "\n",
    "# получаем предсказания и выводим информацию о качестве\n",
    "pred = clf.predict(trigram_vectorized_x_test)\n",
    "print(classification_report(y_test, pred))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VlWxW3e9Wg-m"
   },
   "source": [
    "Как вы думаете, почему в результатах теперь такой разброс по сравнению с униграммами?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u7hCxZRtWg-m"
   },
   "source": [
    "## TF-IDF векторизация"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "`TfidfVectorizer` делает то же, что и `CountVectorizer`, но в качестве значений выдает **tf-idf** каждого слова."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Действуем аналогично, как с ```CountVectorizer()```:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# инициализируем векторизатор, в качестве переменных используем униграммы\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 1))\n",
    "\n",
    "# обучаем его и сразу применяем к x_train\n",
    "tfidf_vectorized_x_train = tfidf_vectorizer.fit_transform(x_train)\n",
    "\n",
    "# инициализируем и обучаем классификатор\n",
    "clf = LogisticRegression(random_state=88, max_iter=1000)\n",
    "clf.fit(tfidf_vectorized_x_train, y_train)\n",
    "\n",
    "# применяем обученный векторизатор к тестовым данным\n",
    "tfidf_vectorized_x_test = tfidf_vectorizer.transform(x_test)\n",
    "\n",
    "# получаем предсказания и выводим информацию о качестве\n",
    "pred = clf.predict(tfidf_vectorized_x_test)\n",
    "print(classification_report(y_test, pred))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D39SSh0zWg-r"
   },
   "source": [
    "Результат не улучшился, поэтому вернемся к `CountVectorizer()`."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Токенизация\n",
    "\n",
    "Токенизировать - значит, поделить текст на части: слова, ключевые слова, фразы, символы и т.д., иными словами **токены**.\n",
    "\n",
    "Самый простой способ токенизировать текст - разделить с помощью функции `split()`. Но `split` упускает очень много всего, например, не отделяет пунктуацию от слов. Кроме этого, есть еще много менее тривиальных проблем, поэтому лучше использовать готовые токенизаторы."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "import nltk # уже знакомая нам библиотека nltk\n",
    "from nltk.tokenize import word_tokenize # готовый токенизатор библиотеки nltk"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-13T13:02:39.543369Z",
     "start_time": "2023-05-13T13:02:39.531844Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Чтобы использовать токенизатор ```word_tokenize```, нужно сначала скачать данные для nltk о пунктуации и стоп-словах. Это просто требование nltk, поэтому просто скачаем требумую информацию:  "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/nascela/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/nascela/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-13T13:02:41.719824Z",
     "start_time": "2023-05-13T13:02:41.286158Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Применим токенизацию:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "['Но', 'не', 'каждый', 'хочет', 'что-то', 'исправлять', ':', '(']"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = 'Но не каждый хочет что-то исправлять:('\n",
    "word_tokenize(example)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-13T13:02:43.780993Z",
     "start_time": "2023-05-13T13:02:43.769635Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Если использовать просто ```split()```, то грустный смайлик :( не отделяется от слова \"исправлять\":"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "['Но', 'не', 'каждый', 'хочет', 'что-то', 'исправлять:(']"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example.split()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-13T13:02:51.698942Z",
     "start_time": "2023-05-13T13:02:51.691206Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "В nltk вообще есть довольно много токенизаторов:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "['BlanklineTokenizer',\n 'LegalitySyllableTokenizer',\n 'LineTokenizer',\n 'MWETokenizer',\n 'NLTKWordTokenizer',\n 'PunktSentenceTokenizer',\n 'RegexpTokenizer',\n 'ReppTokenizer',\n 'SExprTokenizer',\n 'SpaceTokenizer',\n 'StanfordSegmenter',\n 'SyllableTokenizer',\n 'TabTokenizer',\n 'TextTilingTokenizer',\n 'ToktokTokenizer',\n 'TreebankWordDetokenizer',\n 'TreebankWordTokenizer',\n 'TweetTokenizer',\n 'WhitespaceTokenizer',\n 'WordPunctTokenizer',\n '__builtins__',\n '__cached__',\n '__doc__',\n '__file__',\n '__loader__',\n '__name__',\n '__package__',\n '__path__',\n '__spec__',\n '_treebank_word_tokenizer']"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import tokenize\n",
    "dir(tokenize)[:30]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-13T13:02:54.654930Z",
     "start_time": "2023-05-13T13:02:54.647093Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Они умеют выдавать индексы в строке для начала и конца каждого слова-токена:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "[(0, 2), (3, 5), (6, 12), (13, 18), (19, 25), (26, 38)]"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wh_tok = tokenize.WhitespaceTokenizer()\n",
    "list(wh_tok.span_tokenize(example))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-13T13:02:59.672204Z",
     "start_time": "2023-05-13T13:02:59.665188Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Некторые токенизаторы ведут себя специфично:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "['do', \"n't\", 'stop', 'me']"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize.TreebankWordTokenizer().tokenize(\"don't stop me\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-13T13:03:04.112226Z",
     "start_time": "2023-05-13T13:03:04.099268Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "А некоторые -- вообще не для текста на естественном языке:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "['(a (b c))', 'd', 'e', '(f)']"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize.SExprTokenizer().tokenize(\"(a (b c)) d e (f)\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-13T13:03:22.563366Z",
     "start_time": "2023-05-13T13:03:22.555587Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rhVrgkSaWg_K"
   },
   "source": [
    "**Подходящий токенизатор подбирается исходя из требований задачи!**"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Стоп-слова и пунктуация\n",
    "\n",
    "**Стоп-слова** - это слова, которые часто встречаются практически в любом тексте и ничего интересного не говорят о конретном документе. Для модели это просто шум. А шум нужно убирать. По аналогичной причине убирают и пунктуацию."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['и', 'в', 'во', 'не', 'что', 'он', 'на', 'я', 'с', 'со', 'как', 'а', 'то', 'все', 'она', 'так', 'его', 'но', 'да', 'ты', 'к', 'у', 'же', 'вы', 'за', 'бы', 'по', 'только', 'ее', 'мне', 'было', 'вот', 'от', 'меня', 'еще', 'нет', 'о', 'из', 'ему', 'теперь', 'когда', 'даже', 'ну', 'вдруг', 'ли', 'если', 'уже', 'или', 'ни', 'быть', 'был', 'него', 'до', 'вас', 'нибудь', 'опять', 'уж', 'вам', 'ведь', 'там', 'потом', 'себя', 'ничего', 'ей', 'может', 'они', 'тут', 'где', 'есть', 'надо', 'ней', 'для', 'мы', 'тебя', 'их', 'чем', 'была', 'сам', 'чтоб', 'без', 'будто', 'чего', 'раз', 'тоже', 'себе', 'под', 'будет', 'ж', 'тогда', 'кто', 'этот', 'того', 'потому', 'этого', 'какой', 'совсем', 'ним', 'здесь', 'этом', 'один', 'почти', 'мой', 'тем', 'чтобы', 'нее', 'сейчас', 'были', 'куда', 'зачем', 'всех', 'никогда', 'можно', 'при', 'наконец', 'два', 'об', 'другой', 'хоть', 'после', 'над', 'больше', 'тот', 'через', 'эти', 'нас', 'про', 'всего', 'них', 'какая', 'много', 'разве', 'три', 'эту', 'моя', 'впрочем', 'хорошо', 'свою', 'этой', 'перед', 'иногда', 'лучше', 'чуть', 'том', 'нельзя', 'такой', 'им', 'более', 'всегда', 'конечно', 'всю', 'между']\n"
     ]
    }
   ],
   "source": [
    "# импортируем стоп-слова из библиотеки nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# посмотрим на стоп-слова для русского языка\n",
    "print(stopwords.words('russian'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-13T13:03:33.610638Z",
     "start_time": "2023-05-13T13:03:33.587801Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "*Знаки* пунктуации лучше импортировать из модуля **String**. В нем хранятся различные наборы констант для работы со строками (пунктуация, алфавит и др.). "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from string import punctuation\n",
    "punctuation"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-13T13:03:48.459731Z",
     "start_time": "2023-05-13T13:03:48.445784Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Объединим стоп-слова и знаки пунктуации вместе и запишем в переменную ```noise```:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "noise = stopwords.words('russian') + list(punctuation)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-13T13:03:58.619986Z",
     "start_time": "2023-05-13T13:03:58.590323Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Теперь нужно обучить нашу модель с учетом новых знаний про токенизацию и стоп-слова. \n",
    "\n",
    "Для этого мы можем собрать новый векторизатор, передав ему на вход:\n",
    "* какие n-граммы нам нужны, параметр **ngram_range**;\n",
    "* какой токенизатор мы используем, параметр **tokenizer**;\n",
    "* какие у нас стоп-слова, параметр **stop_words**.\n",
    "\n",
    "*Напоминание:* мы используем готовый токенизатор ```word_tokenize```, а стоп-слова хранятся в переменной ```noise```"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 379
    },
    "executionInfo": {
     "elapsed": 252716,
     "status": "ok",
     "timestamp": 1595571957270,
     "user": {
      "displayName": "Viacheslav Ivanov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj2nVSPc3MmvxSyrr0vJY2olNGRMXHUnFlFUnxktA=s64",
      "userId": "14095316471752402401"
     },
     "user_tz": -180
    },
    "id": "7Nc6D-nwWg_P",
    "outputId": "22ab3aef-053b-447b-b784-c46f990353db",
    "ExecuteTime": {
     "end_time": "2023-05-13T13:04:34.830005Z",
     "start_time": "2023-05-13T13:04:34.812375Z"
    }
   },
   "outputs": [],
   "source": [
    "# инициализируем умный векторайзер \n",
    "smart_vectorizer = CountVectorizer(ngram_range=(1, 1), \n",
    "                                   tokenizer=word_tokenize, \n",
    "                                   stop_words=noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nascela/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/nascela/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['``'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.80      0.79     22480\n",
      "    positive       0.80      0.76      0.78     22887\n",
      "\n",
      "    accuracy                           0.78     45367\n",
      "   macro avg       0.78      0.78      0.78     45367\n",
      "weighted avg       0.78      0.78      0.78     45367\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# обучаем его и сразу применяем к x_train\n",
    "smart_vectorized_x_train = smart_vectorizer.fit_transform(x_train)\n",
    "\n",
    "# инициализируем и обучаем классификатор\n",
    "clf = LogisticRegression(random_state=42, max_iter=1000)\n",
    "clf.fit(smart_vectorized_x_train, y_train)\n",
    "\n",
    "# применяем обученный векторайзер к тестовым данным\n",
    "smart_vectorized_x_test = smart_vectorizer.transform(x_test)\n",
    "\n",
    "# получаем предсказания и выводим информацию о качестве\n",
    "pred = clf.predict(smart_vectorized_x_test)\n",
    "print(classification_report(y_test, pred))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-13T13:25:57.187400Z",
     "start_time": "2023-05-13T13:25:29.132796Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Результат стал немного лучше: accuracy выше, а также заметно подрос recall у негативного класса."
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "VlWxW3e9Wg-m",
    "D39SSh0zWg-r",
    "rhVrgkSaWg_K",
    "XsRf9T_SWg_U",
    "ylKZG2MwWg_f",
    "9hedBdcYWhAH",
    "JrqW55jgWhAR",
    "5QYTwyMtWhAZ",
    "DbJrUpARWhAd",
    "MI18l-l9WhAk",
    "1wrEGqBSWhAr",
    "gStgBJy2WhAx"
   ],
   "name": "Копия блокнота \"seminar_1_preprocessing.ipynb\"",
   "provenance": [
    {
     "file_id": "1TtILmuSoWXOYmbTIAQmGaScvuHGWvpsI",
     "timestamp": 1595574229609
    },
    {
     "file_id": "1EdBdyqxLu-WiLmriWNwYl5Ct33JYcEG2",
     "timestamp": 1582113683695
    },
    {
     "file_id": "10_Aehfbxgr3fxXPgI1gM5BTU8yOy-Z4U",
     "timestamp": 1579514615233
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
